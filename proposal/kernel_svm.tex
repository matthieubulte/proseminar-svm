\section {Kernel methods}

The previous section introduced the large margin classifier. The large margin classifier, as we saw, allows us the find the best seperating hyperplane between two linearly seperable classes of a dataset. In this section we introduce the concept of kernels, allowing us to learn non linear decision boudaries, continuing with the Mercer theorem and kernel trick, results from functional analysis allowing us to use the newly introducd ideas of kernels with the previously introduces agorithm, with only light modifications.

\subsection{Non linearly separable data}

Very often, the decision boundary one is trying to learn is in it's nature non-linear. One trick that is often used in order to use linear learning algorithms is to add one or several dimensions to the data points by applying a non linear function of the coordinates of the point, to then learn the separating hyperplane in the projected space. This method is known as features engineering, because we add engineered features to our data points.


This approach would be very simple to implement in our current algorithm. Let $\phi : \mathcal{X} \rightarrow \mathcal{V}$ be a non linear transformation from our original space $\mathcal{X}$ to some higer dimensional Hilbert space $\mathcal{V}$. We can now modify the previously introduced algorithm by replacing simply every scalar product $\left<\cdot , \cdot\right>$ by the scalar product on the projected points in the $\mathcal{V}$ space, namely $\left<\phi(\cdot), \phi(\cdot)\right>$.


In order to better understand kernels and the kernel trick, let's follow the example of polynomial projection of degree $d$, in which every point is projected to a vector containing every monomial of degree $d$. For instance, choosing $d = 2$ together with a $2$ dimensional input space leads to the projection $\phi : \mathbb{R}^2 \rightarrow \mathbb{R}^4$ defined as

\begin{equation*}
    \phi(x_1, x_2) \mapsto (x_1^2, x_2^2, x_1x_2, x_2x_1)
\end{equation*}

This representation, while adding more separation capabilities has the problem that the image feature space grows at an exponential rate together with $d$, making the choice of a larger $d$ prohibitivly expensive in terms of space usage. Though, the margin maximization algorithm only uses the scalar product of the observations in the feature space, and the following equations show that the projection in the high dimensional space is not required to the computation of the scalar product

\begin{equation*}
  \begin{aligned}
    \left<\phi(x), \phi(x')\right>
    &= [x]_1^2[x']_1^2 + [x]_2^2[x']_2^2 + 2[x]_1[x]_2[x']_1[x']_2\\
    &= \left<x, x'\right>^2 \\
    &=: k(x, x')
  \end{aligned}
\end{equation*}

We call $k(\cdot, \cdot) : \mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}$ the kernel representation of the scalar product in the space $\phi(\mathbb{R}^2)$. One can generalize this idea to any polynomial degree $d$ as shown in \textcolor[rgb]{1,0,0}{ref?}, making the computation of the scalar product in the $d$-th monomial space as trivial as computing the scalar product in the original space.

\subsection{Kernel trick}

We have seen in the previous example, that it is sometimes possible to find a function $k :\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ with $k(x, x') = \left<\phi(x), \phi(x')\right>$ for some projection $\mathcal{X} \rightarrow \mathcal{V}$. With these special functions, it is possible to then, only lightly modifying our original algorithm, to run our learning algorithm in another vector space without the computational cost of projecting our data in this other space. This is called the kernel trick.

We will now change our point of view, and try to define the properties defining the class of functions being the representation of a scalar product of the projection of its inputs in another vector space. This means, which properties must hold for $k$ in order for a $\phi$ to exist with the correspondance property. \textcolor[rgb]{1,0,0}{better name needed, also a definition would be nice} 

Let $k :\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ with $k(x, x') = \left<\phi(x), \phi(x')\right>_{\mathcal{V}}$ for some projection $\mathcal{X} \rightarrow \mathcal{V}$, because $\left<\cdot, \cdot\right>_{\mathcal{V}}$ is a scalar product, the following properties must hold

\begin{itemize}
\item \textit{Symmetry}
  \begin{equation*}
    \begin{aligned}
      & \forall y_1, y_2 \in \mathcal{V}.\ 
      \left<y_1, y_2\right>_{\mathcal{V}} = \overline{\left<y_2, y_1\right>_{\mathcal{V}}} & \Rightarrow\\
      & \forall y_1, y_2 \in \phi(\mathcal{X}).\ 
      \left<y_1, y_2\right>_{\mathcal{V}} = \overline{\left<y_2, y_1\right>_{\mathcal{V}}} & \Rightarrow\\
      &\forall x_1, x_2 \in \mathcal{X}.\ 
      \left<\phi(x_1), \phi(x_2)\right>_{\mathcal{V}} = \overline{\left<\phi(x_2), \phi(x_1)\right>_{\mathcal{V}}} &\Rightarrow\\
      &\forall x_1, x_2 \in \mathcal{X}.\ 
         k\left(x_1, x_2\right) = \overline{k\left(x_2, x_1\right)}
    \end{aligned}
  \end{equation*}

\item \textit{Positive definitness}
  \begin{equation*}
    \begin{aligned}
      & \forall y \in \mathcal{V}.\ 
        \left<y, y\right>_{\mathcal{V}} \geq 0 & \Rightarrow\\
      & \forall y \in \phi(\mathcal{X}).\ 
        \left<y, y\right>_{\mathcal{V}} \geq 0 & \Rightarrow\\
      &\forall x \in \mathcal{X}.\ 
        \left<\phi(x), \phi(x)\right>_{\mathcal{V}} \geq 0 & \Rightarrow\\
      &\forall x \in \mathcal{X}.\ 
        k\left(x, x\right) \geq 0
    \end{aligned}
  \end{equation*}
\end{itemize}

We will show by construction, that these properties are sufficient to the proof of the existence of the desired space.  

The literature contains several examples of vector spaces and projections with the desired property. We will here proove the existence of such a space by constructing the Reproducing Kernel Hilbert Space and a projection into it.



While being an approachable and sufficient prove of existence, the interested reader is invited to read about the Mercer Kernel Map, which is most often used to prove properties of the algorithm \textcolor[rgb]{1,0,0}{ref?}.







 \textcolor[rgb]{1,0,0}{ maybe we want to introduce the idea that a scalar product is a way to measure similarity between vectors, thus kernels can be seen as a different way to measure similarity between observatiosn. Kernels contain domain knowledge and can enforce invariants }


\subsection {Some useful kernels}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_with_kernels"
%% End:

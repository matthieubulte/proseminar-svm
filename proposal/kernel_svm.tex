\section {Kernel methods}

The previous section introduced the large margin classifier. The large margin classifier, as we saw, allows us the find the best seperating hyperplane between two linearly seperable classes of a dataset. In this section we introduce the concept of kernels, allowing us to learn non linear decision boudaries, continuing with the Mercer theorem and kernel trick, results from functional analysis allowing us to use the newly introducd ideas of kernels with the previously introduces agorithm, with only light modifications.

\subsection{Non linearly separable data}

Very often, the decision boundary one is trying to learn is in it's nature non-linear. One trick that is often used in order to use linear learning algorithms is to add one or several dimensions to the data points by applying a non linear function of the coordinates of the point, to then learn the separating hyperplane in the projected space. This method is known as features engineering, because we add engineered features to our data points.


This approach would be very simple to implement in our current algorithm. Let $\phi : \mathcal{X} \rightarrow \mathcal{V}$ be a non linear transformation from our original space $\mathcal{X}$ to some higer dimensional Hilbert space $\mathcal{V}$. We can now modify the previously introduced algorithm by replacing simply every scalar product $\left<\cdot , \cdot\right>$ by the scalar product on the projected points in the $\mathcal{V}$ space, namely $\left<\phi(\cdot), \phi(\cdot)\right>$.


In order to better understand kernels and the kernel trick, let's follow the example of polynomial projection of degree $d$, in which every point is projected to a vector containing every monomial of degree $d$. For instance, choosing $d = 2$ together with a $2$ dimensional input space leads to the projection $\phi : \mathbb{R}^2 \rightarrow \mathbb{R}^4$ defined as

\begin{equation*}
    \phi(x_1, x_2) \mapsto (x_1^2, x_2^2, x_1x_2, x_2x_1)
\end{equation*}

This representation, while adding more separation capabilities has the problem that the image feature space grows at an exponential rate together with $d$, making the choice of a larger $d$ prohibitivly expensive in terms of space usage. Though, the margin maximization algorithm only uses the scalar product of the observations in the feature space, and the following equations show that the projection in the high dimensional space is not required to the computation of the scalar product

\begin{equation*}
  \begin{aligned}
    \left<\phi(x), \phi(x')\right>
    &= [x]_1^2[x']_1^2 + [x]_2^2[x']_2^2 + 2[x]_1[x]_2[x']_1[x']_2\\
    &= \left<x, x'\right>^2 \\
    &=: k(x, x')
  \end{aligned}
\end{equation*}

We call $k(\cdot, \cdot) : \mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}$ the kernel representation of the scalar product in the space $\phi(\mathbb{R}^2)$. One can generalize this idea to any polynomial degree $d$ as shown in \textcolor[rgb]{1,0,0}{ref?}, making the computation of the scalar product in the $d$-th monomial space as trivial as computing the scalar product in the original space.

\subsection{Kernel trick}

We have seen in the previous example, that it is sometimes possible to find a function $k :\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ with $k(x, x') = \left<\phi(x), \phi(x')\right>$ for some projection $\mathcal{X} \rightarrow \mathcal{V}$. With these special functions, it is possible to then, only lightly modifying our original algorithm, to run our learning algorithm in another vector space without the computational cost of projecting our data in this other space. This is called the kernel trick.

We will now change our point of view, and try to define the properties defining the class of functions being the representation of a scalar product of the projection of its inputs in another vector space. This means, which properties must hold for $k$ in order for a $\phi$ to exist with the correspondance property. \textcolor[rgb]{1,0,0}{better name needed, also a definition would be nice} 

Let $k :\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ with $k(x, x') = \left<\phi(x), \phi(x')\right>_{\mathcal{V}}$ for some projection $\mathcal{X} \rightarrow \mathcal{V}$, because $\left<\cdot, \cdot\right>_{\mathcal{V}}$ is a scalar product, the following properties must hold

\begin{itemize}
\item \textit{Symmetry}
  \begin{equation*}
    \begin{aligned}
      & \forall y_1, y_2 \in \mathcal{V}.\ 
      \left<y_1, y_2\right>_{\mathcal{V}} = \overline{\left<y_2, y_1\right>_{\mathcal{V}}} & \Rightarrow\\
      & \forall y_1, y_2 \in \phi(\mathcal{X}).\ 
      \left<y_1, y_2\right>_{\mathcal{V}} = \overline{\left<y_2, y_1\right>_{\mathcal{V}}} & \Rightarrow\\
      &\forall x_1, x_2 \in \mathcal{X}.\ 
      \left<\phi(x_1), \phi(x_2)\right>_{\mathcal{V}} = \overline{\left<\phi(x_2), \phi(x_1)\right>_{\mathcal{V}}} &\Rightarrow\\
      &\forall x_1, x_2 \in \mathcal{X}.\ 
         k\left(x_1, x_2\right) = \overline{k\left(x_2, x_1\right)}
    \end{aligned}
  \end{equation*}

\item \textit{Positive definitness} Positive definitness of a kernel is a stronger property than the one required for the positive definiteness in the features space. A kernel is said to be positive definite if for every $\alpha_1, ..., \alpha_n \in \mathbb{R}$ following inequality holds
  \begin{equation*}
    \sum_{i,j=1}^n\alpha_i\alpha_jk\left(x_i, x_j\right) \geq 0
  \end{equation*}
\end{itemize}

We will show by construction, that these properties are sufficient to the proof of the existence of the desired space. The literature contains several examples of vector spaces and projections with the desired property. We will here proove the existence of such a space by construction.

Let $k : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be symmetrical and positive definite, and $\mathbb{R}^{\mathcal{X}}$ be the set of functions from a non empty set $\mathcal{X}$ to $\mathbb{R}$. We define the reproducing kernel map as follow

\begin{equation}
  \begin{aligned}
    \phi : \mathcal{X} \rightarrow \mathcal{H}\\
    x \mapsto k(\cdot, x)
  \end{aligned}
\end{equation}

We now show how the mapped observations of the training set $\left\{k\left(\cdot, x_1\right), ..., k\left(\cdot, x_n\right)\right\}$ spans a Hilbert space in which the reproducing property hold with the scalar defined on the span set $\left<k\left(\cdot, x_i\right), k\left(\cdot, x_j\right)\right> = k\left(x_i, x_j\right)$ and its canonical derivation for the spanned space

\begin{equation*}
  \begin{aligned}
    \left<f, g\right>
    &= \left<\sum_{i=1}^n\alpha_ik(\cdot, x_i), \sum_{j=1}^n\beta_jk(\cdot, x_j)\right>\\
    &= \sum_{i,j=1}^n\alpha_i\beta_j \left<k\left(\cdot, x_i\right), k\left(\cdot, x_j\right)\right> \\
    &= \sum_{i,j=1}^n\alpha_i\beta_j k\left(x_i, x_j\right)
  \end{aligned}
\end{equation*}

Positive definitness, bilinearity and symmetry can all be trivially derived from the definition of this scalar product together with the symmetry and positive definitness of the kernel function. Then attentive reader will notive that the functions $\left\{k\left(\cdot, x_1\right), ..., k\left(\cdot, x_n\right)\right\}$ must nost necessarily be linearly independant, leading to a non unqiue mapping from $f$ to the coefficients $\alpha_1, ..., \alpha_n$. To see that this doesn't imply the ill definitness of the defined scalar product, we note that

\begin{equation*}
  \left<f, g\right> = \sum_{j=1}^n\beta_j f\left(x_j\right)
\end{equation*}

and by symmetry

\begin{equation*}
  \left<f, g\right> = \left<g, f\right> = \sum_{i=1}^n\alpha_i g\left(x_i\right)
\end{equation*}

Which shows that the scalar product does not depend on the choice of coefficients for the functions $f$ and $g$ confirming the scalar product is well defined. Thus, the previously defined scalar product, together with the norm $|f| = \sqrt{\left<f, f\right>}$ define a valid Hilbert space and the correspondance property holds

\begin{equation*}
  \begin{aligned}
    \left<\phi(x_i), \phi(x_j)\right> =\ &\left<k\left(\cdot, x_i\right), k\left(\cdot, x_j\right)\right>\\
    =\ &k(x_i, x_j)
  \end{aligned}
\end{equation*}

Now that we have shown the kernel trick as well as the existance of a corresponding Hilbert space, the next section will focus on studying the benefits of some chosen kernels that are often used in practical applications.

\subsection {Some useful kernels}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_with_kernels"
%% End:

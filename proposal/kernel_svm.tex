\begin{figure*}
  \begin{minipage}{.5\textwidth}
    \centering
    \input{fig3}
    %\includegraphics[width=\linewidth]{influence_map.png}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{decision_boundary.png}
  \end{minipage}
  \caption{
    Le left pane illustrates how a non linear projection in a higher dimensional space makes $(1)$ it possible to learn non linear decisions boundaries in the observations space by projecting back the learned linear decision boundary $(2)$. On the right pane, the solid line represents the decision boundary and the dashed lines represents the contour levels of the decision function with learned using an RBF kernel.
  }
\end{figure*}

\section{Kernel methods}

The previous section introduced the linear SVM classifier. As we have seen, the linear SVM  allows us the find the best separating hyperplane between two linearly separable classes of observations. In this section we introduce the concept of kernels, allowing us to learn non linear decision boundaries, continuing with Mercer's theorem and the kernel trick, results from functional analysis allowing us to use the newly introduced ideas of kernels with the previously presented algorithm, with only light modifications.

\subsection{Non linearly separable data}

Very often, the decision boundary one is trying to learn is in its nature non-linear. One method that is often used to make an hyperplane learning algorithm learn a non-linear decision boundary is to first map the dataset to another space using a non linear transformation. The learned boundary is then linear in the mapped space, but because the chosen projection is not linear, the pre-image of the decision boundary is not linear.

This approach is very simple to implement and only requires light modifications the presented algorithm. Let $\phi : \mathbb{R}^n \rightarrow \mathcal{V}$ be a non linear transformation to a pre-Hilbert space $\mathcal{V}$. A pre-Hilbert space is a generalization of an Euclidean space, the geometrical space we are used to. Saying that the image of the projection should be a pre-Hilbert space simply means that the image set is equipped with a scalar product, needed for our training algorithm. We can now modify the previously introduced algorithm by replacing simply every dot product by the scalar product on the projected points in the $\mathcal{V}$ space, namely $\langle \phi(\cdot), \phi(\cdot)\rangle _{\mathcal{V}}$.

In order to better understand kernels and the kernel trick, let's analyze the example of polynomial projection of degree $d$, in which every point is projected to a vector containing every monomial of degree $d$. For instance, choosing $d = 2$ together with a $2$ dimensional input space leads to the projection $\phi : \mathbb{R}^2 \rightarrow \mathbb{R}^3$ defined as

\begin{equation*}
    \phi(x_1, x_2) \mapsto (x_1^2, x_2^2, \sqrt{2}x_1x_2)
\end{equation*}

This projection, illustrated in figure 2, makes it possible to learn a decision boundary that is not linear in the observations but it has the problem that the dimension of the image space grows at an exponential rate together with $d$, making the choice of a larger $d$ prohibitively expensive. Though, the margin maximization algorithm only uses the scalar product of the observations in the feature space, which is simply the dot product of the mapped vectors. The following equations show that the projection in the higher dimensional space is not required to the computation of the scalar product, and can be replaced by a function of the two vectors in the original space

\begin{equation*}
  \begin{aligned}
    \phi(x) \cdot \phi(\tilde{x})
    &= x_1^2\tilde{x}_1^2 + x_2^2\tilde{x}_2^2 + \sqrt{2}x_1x_2\sqrt{2}\tilde{x}_1\tilde{x}_2\\
    &= x_1^2\tilde{x}_1^2 + x_2^2\tilde{x}_2^2 + 2x_1x_2\tilde{x}_1\tilde{x}_2\\
    &= \left(x \cdot \tilde{x}\right)^2 \\
    &=: k(x, \tilde{x})
  \end{aligned}
\end{equation*}

We call $k(\cdot, \cdot) : \mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}$ the kernel representation of the scalar product in the space $\phi(\mathbb{R}^2)$. One can show that such a function exists for any polynomial degree $d$, making the computation of the scalar product in the $d$-th monomial space as trivial as computing the scalar product in the original space.

\subsection{Kernel trick}

We have seen in the previous example, that it is sometimes possible to find a function $k :\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ with $k(x, x') = \langle \phi(x), \phi(x')\rangle _{\mathcal{V}}$ for some projection $\phi : \mathbb{R}^n \rightarrow \mathcal{V}$. With these special functions, it is possible by only lightly modifying our original algorithm to run the learning algorithm in another pre Hilbert space without the computational cost of projecting our data in this other space. This is called the kernel trick.

We will now change our point of view, and try to define the properties defining the class of functions being the representation of a scalar product of the projection of its inputs in another vector space. This means, which properties must hold for $k$ in order for a $\phi$ to exist with the reproducing property.

Let $k :\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ with $k(x, x') = \langle \phi(x), \phi(x')\rangle _{\mathcal{V}}$ for some projection $\mathbb{R}^n \rightarrow \mathcal{V}$, because $\langle \cdot, \cdot\rangle _{\mathcal{V}}$ is a scalar product, the following properties must hold

\begin{itemize}
\item \textit{Symmetry}
  \begin{equation*}
    \begin{aligned}
      & \forall y_1, y_2 \in \mathcal{V}.\ 
      \langle y_1, y_2\rangle _{\mathcal{V}} = \overline{\langle y_2, y_1\rangle _{\mathcal{V}}} & \Rightarrow\\
      & \forall y_1, y_2 \in \phi(\mathbb{R}^n).\ 
      \langle y_1, y_2\rangle _{\mathcal{V}} = \overline{\langle y_2, y_1\rangle _{\mathcal{V}}} & \Rightarrow\\
      &\forall x_1, x_2 \in \mathbb{R}^n.\ 
      \langle \phi(x_1), \phi(x_2)\rangle _{\mathcal{V}} = \overline{\langle \phi(x_2), \phi(x_1)\rangle _{\mathcal{V}}} &\Rightarrow\\
      &\forall x_1, x_2 \in \mathbb{R}^n.\ 
         k\left(x_1, x_2\right) = \overline{k\left(x_2, x_1\right)}
    \end{aligned}
  \end{equation*}

\item \textit{Positive definiteness} Positive definiteness of a kernel is a stronger property than the one required for the positive definiteness in the features space. A kernel is said to be positive definite if for every $\alpha_1, ..., \alpha_n \in \mathbb{R}$ following inequality holds
  \begin{equation*}
    \sum_{i,j=1}^n\alpha_i\alpha_jk\left(x_i, x_j\right) \geq 0
  \end{equation*}
\end{itemize}

We will show by construction, that these properties are sufficient to the proof of the existence of the desired space. The literature contains several examples of vector spaces and projections with the desired property. We will here prove the existence of such a space by construction.

Let $k : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ be symmetrical and positive definite, and $\mathbb{R}^{(\mathbb{R}^n)}$ be the set of functions from $\mathbb{R}^n$ to $\mathbb{R}$. We define the reproducing kernel map as follow

\begin{equation}
  \begin{aligned}
    \phi : \mathbb{R}^n \rightarrow \mathcal{H}\\
    \mathbf{x} \mapsto k(\cdot, \mathbf{x})
  \end{aligned}
\end{equation}

We now show how the mapped observations of the training set $\left\{k\left(\cdot, \mathbf{x}_1\right), ..., k\left(\cdot, \mathbf{x}_n\right)\right\}$ spans a Hilbert space in which the reproducing property hold with the scalar product defined on the span set $\langle k\left(\cdot, \mathbf{x}_i\right), k\left(\cdot, \mathbf{x}_j\right)\rangle = k\left(\mathbf{x}_i, \mathbf{x}_j\right)$ and its canonical derivation for the spanned space

\begin{equation*}
  \begin{aligned}
    \langle f, g\rangle 
    &= \langle \sum_{i=1}^n\alpha_ik(\cdot, \mathbf{x}_i), \sum_{j=1}^n\beta_jk(\cdot, \mathbf{x}_j)\rangle \\
    &= \sum_{i,j=1}^n\alpha_i\beta_j \langle k\left(\cdot, \mathbf{x}_i\right), k\left(\cdot, \mathbf{x}_j\right)\rangle \\
    &= \sum_{i,j=1}^n\alpha_i\beta_j k\left(\mathbf{x}_i, \mathbf{x}_j\right)
  \end{aligned}
\end{equation*}

Positive definiteness, bilinearity and symmetry can all be trivially derived from the definition of this scalar product together with the symmetry and positive definiteness of the kernel function. The attentive reader will have notices that the functions $\left\{k\left(\cdot, x_1\right), ..., k\left(\cdot, x_n\right)\right\}$ must not necessarily be linearly independent, leading to a non unique mapping from $f$ to the coefficients $\alpha_1, ..., \alpha_n$. To see that this doesn't imply the ill-definiteness of the defined scalar product, we note that

\begin{equation*}
  \langle f, g\rangle = \sum_{j=1}^n\beta_j f\left(\mathbf{x}_j\right)
\end{equation*}

and by symmetry

\begin{equation*}
  \langle f, g\rangle = \langle g, f\rangle = \sum_{i=1}^n\alpha_i g\left(\mathbf{x}_i\right)
\end{equation*}

Which shows that the scalar product does not depend on the choice of coefficients for the functions $f$ or $g$ confirming the scalar product is well defined. Thus, the previously defined scalar product, together with the norm $\|f\| = \sqrt{\langle f, f\rangle }$ define a valid Hilbert space and the reproducing property holds

\begin{equation*}
  \begin{aligned}
    \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j)\rangle =\ &\langle k\left(\cdot, \mathbf{x}_i\right), k\left(\cdot, \mathbf{x}_j\right)\rangle \\
    =\ &k(\mathbf{x}_i, \mathbf{x}_j)
  \end{aligned}
\end{equation*}

Now that we have presented the kernel trick as well as the existence of a corresponding pre-Hilbert space, the next section will focus on finding and creating useful kernels, as well as presenting some classical kernels.

\subsection {Some useful kernels}

We have defined in the last section what kernels are and how we can modify our algorithm to learn non-linear boundaries in our data. What we have omitted so far is how is one supposed to choose develop or choose a kernel when facing a practical problem.

One first way to choose a kernel is by using existing knowledge about the shape the decision boundary should have. Let's recall the equation of the decision boundary defined by the learned parameters using the kernel $k$

\begin{equation*}
  \mathscr{H} = \left\{\mathbf{x} \in \mathbb{R}^n\ |\ \sum_{\mathbf{x}_i \text{ is SV}}\alpha_iy_ik\left(\mathbf{x}, \mathbf{x}_i\right) + b = 0\right\}
\end{equation*}

For illustration, if we choose the kernel to be a polynomial kernel of degree $2$, meaning $k\left(\mathbf{x}, \mathbf{\tilde{x}}\right) = \left(\mathbf{x} \cdot \mathbf{\tilde{x}} + 1\right)^2$, we can refine the definition of the decision boundary as the set of every $\mathbf{x} \in \mathbb{R}^2$ for which following holds

\begin{equation*}
  \begin{aligned}
    0 &=\ \left(w \cdot \mathbf{x} + 1\right)^2 + b\\
      &=\ \left(w_1x_1 + w_2x_2 + 1\right)^2 + b\\
      &=\ (w_1x_1)^2 + (w_2x_2)^2 + 2w_1x_1w_2x_2 + w_1x_1 + w_2x_2 + 1 + b\\
      &=\ \mathbf{x}^T\begin{bmatrix}w_1^2 & w_1w_2\\ w_1w_2 & w_2^2\end{bmatrix}\mathbf{x} + w \cdot \mathbf{x} + 1 
  \end{aligned}
\end{equation*}

The decision boundary formed by this equation is called a quadric, the generalization of a conic section, which takes a shape from a known set of different kind of solutions. Thus, if one knows a quadric decision boundary is needed, then using a polynomial kernel of degree $2$ will learn the proper parameters of the quadric.

We have seen how previous knowledge on the shape of the decision boundary can lead to the choice of a particular kernel. Unfortunately, it is often the case that the best kind of decision boundary can't be determined. Thankfully, there is still a way to use domain knowledge about the problem to choose a kernel that will best help training a performant classifier.


In order to incorporate this other kind of domain knowledge, we have to look at kernels, not as the computations of a scalar product in some feature space, but as a similarity measurement. Taking this point of view, the kernel trick becomes a way to find the proper Hilbert space in which one's definition of similarity defines a scalar proper product, letting us use the Support Vector Machine machinery as a way to train a classifier based on our definition of similarity.

What do we mean by similarity? Let's first write down a more geometrical  definition of the euclidean scalar product in $\mathbb{R}^n$, the inner product 

\begin{equation*}
  \mathbf{x} \cdot \mathbf{\tilde{x}} = \|\mathbf{x}\|\|\mathbf{\tilde{x}}\|\text{cos}\left(angle\left(\mathbf{x}, \mathbf{\tilde{x}}\right)
  \right)
\end{equation*}

This equivalent definition of the euclidean scalar product makes it more visible what the underlying notion of similarity of the euclidean scalar product is measured by the angle between the two vectors, scaled by their length. A similar kernel often used is the cosine similarity kernel, defined as

\begin{equation*}
  k(\mathbf{x}, \mathbf{\tilde{x}}) = \frac{\mathbf{x} \cdot \mathbf{\tilde{x}}}{\|\mathbf{x}\|\|\mathbf{\tilde{x}}\|} = \text{cos}\left(angle\left(\mathbf{x}, \mathbf{\tilde{x}}\right)\right)
\end{equation*}

This notion of similarity will thus compare observations by first normalizing them to then compare the directions of the vectors. This similarity measure is often used in the context of text classification, where each entry of the vector corresponds to the number of occurences of a word in a text. The normalization will transform count of occurences in frequencies, thus having a similarity based on the relative importance of each word, independently of the length of the text.

Cosine similarity is an interesting similarity, but one natural way to reason about similarity of points is their distance from one another. The next kernel we introduce incorporates the notions of distance as a similarity measurement, is called the Radial Basis Function kernel and is defined as followed for some hyperparameter $\sigma \in \mathbb{R}$

\begin{equation*}
  k(\mathbf{x}, \mathbf{\tilde{x}}) = \text{exp}\left(-\frac{\|\mathbf{x} - \mathbf{\tilde{x}}\|^2}{\sigma^2}\right)
\end{equation*}

This kernel contains the idea that similarity between two observations should decay exponentially with the distance between the two vectors. This kernel isn't only a good model of our notion of similarity, but it also has interesting properties when used with the Support Vector Machines training algorithm.

Let's first recall what was the decision function that was learned by the algorithm

\begin{equation*}
  f\left(x\right) = sgn\left(\sum_{\mathbf{x}_i \text{ is SV}}\alpha_iy_ik\left(\mathbf{x}, \mathbf{x}_i\right) + b\right)
\end{equation*}

We evaluate the sign of a linear combination of the similarity of the current observation to the support vectors. One can see understand each support vector as a point of influence in the direction of the class it belongs to, with it's influence intensity exponentially decaying with the distance. The observation will then be classified by taking a weighted sum of the influence of each support vector and verifying if it is over or under some threshold $b$.

We have shown with three examples how previous knowledge can help train high quality classifiers. Because we had to omit many interesting concepts of choice of kernel, the reader is invited to consult further resources to learn about more mechanical ways to construct kernels \textcolor[rgb]{1,0,0}{ref linear combination, partial kernels...} but also about invariane incorporation \textcolor[rgb]{1,0,0}{ref} or advanced approach in kernel design for solving modern complex problems \textcolor[rgb]{1,0,0}{ref}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_with_kernels"
%%% End:

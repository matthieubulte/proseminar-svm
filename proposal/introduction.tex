\section{Introduction}

Machine learning can be seen as the science of automatically finding regularities or patterns in data. The contribution from many fields of science, engineering and mathematics brought a large variety of approaches to solving this problem.

In this work, we will bring our attention to the subject of kernel algorithms with a focus on support vector machines. While most of the focus nowadays goes into neural networks, support vector machine stays a relevant algorithm with useful theoretical, but also practical properties that we will study.

The first section will focus on presenting on a simplified version of the suppor vector machine: the hard margin classifier. We will show how the algorithm constructs the optimal hyperplane for separating two classes of observations. This will help construct the necessary intuition to understand classical extensions to the algorithm as well as the kernelization of the algorithm.

In the second second, we will study kernel methods and the kernel trick. A method for working with a projection of the data in a higher dimensional space without having to pay the computational cost of applying the projection. We will concluse the section with an overview of different kernels, an discuss how kernels helps us to incroporate domain knowledge to the learned classifier.

We will then conclude this paper with an overview of practical challenges that can be found while using support vector machines on modern data sets, and mention interesting extensions to support vector machines that were added in the 50 years of their existence.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_with_kernels"
%%% End:

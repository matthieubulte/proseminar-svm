\section {Margin maximization classifier}

The hard margin classifier learns the parameters $w \in \mathbb{R}^n$ and $b \in \mathbb{R}$ defining the hyperplane $\mathscr{H} := \left\{ \mathbf{x} \in \mathbb{R}^n\ |\ w \cdot \mathbf{x} + b = 0\right\}$, separating two classes of observations $\mathbf{x_i} \in \mathbb{R}^n$. To train this classifier, the algorithm takes a set of $m$ observations $\mathbf{x_i}$ together with a vector of labels $y_i = \pm 1$ for each of the observations indicating which class each observation belongs to. 

\begin{equation*}
(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dotsc, (\mathbf{x}_m, y_m)
\end{equation*}

The learned parameters are then used to define a decision function $f$, assigning a label $\pm 1$ to points of $\mathbb{R}^n$ corresponding to the side of the hyperplane on which the observation stands

\begin{equation}
  f(\mathbf{x}) = sgn(w \cdot \mathbf{x} + b)
\end{equation}

\begin{figure*}
  \begin{minipage}{.5\textwidth}
    \centering
    \input{fig1}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \input{fig2}
  \end{minipage}

  \caption{
    Left side illustrates the idea behind margin maximization. Smaller points in the $\{ \mathbf{x} + M \}$ are generated by adding noise to an observation of the training set. The right figure shows a dataset separated by a hyperplane with parameters $w, b$. The norm of $w$ is determined by the distance from the plane to the support vectors.
  }
\end{figure*}

\subsection {Maximizing the margin}

One particularity about the SVM's learning algorithm is the objective function it tries to optimize. Other learning algorithms typically choose a loss function based on the empirical risk, defined for a classifier $f$ and a training set $(\mathbf{x}, \mathbf{y})$ of size $m$ as following

\begin{equation}
  R_{\text{emp}}[f] = \frac{1}{m}\sum^m_{i=1}\frac{1}{2}|f(\mathbf{x}_i) - y_i|
\end{equation}

The margin maximization algorithm instead searches within the space of hyperplanes properly classifying the training set for the hyperplane with the largest margin: distance from the training points to the hyperplane.

The choice of maximizing the margin has good theoretical roots. Indeed, it is possible to derive an upper bound on the expected misclassification rate of a linear classifier based on the margin of the underlying hyperplane to the training set. It is beyond the scope of this work to go into further details but we will still attempt to understand why maximizing the margin can lead to better generalization. For more information the reader can refer to \cite{Vapnik:1995:NSL:211359}.

The intuition comes from the fact that the observations from the test set, or encountered while using the classifier, have been generated by a similar process as the observations present in the training set. Thus, one can assume that observations are very similar to each other up to some noise. Maximizing the hyperplane margin $M$ is thereby equivalent to increasing the classifier's tolerance to noise. As shown in figure 1, given an observation $\mathbf{x}_i$ from the training set, all new observations lying within the hypersphere of radius of the margin will be classified similarly to $\mathbf{x}_i$.

This idea translates well into the following optimization problem, trying to maximize the margin while constraining the search space to the set of classifiers properly classifying the training set

\begin{equation}
  \begin{aligned}
    &\underset{w \in \mathbb{R}^n, b \in \mathbb{R}} {\text{maximize}}
    & & M := \frac{1}{\|w\|} \left[\underset{i} {\text{min}}\ 
    y_i(w \cdot \mathbf{x}_i + b)\right]\\
    &\text{subject to}
    & &y_i(w \cdot \mathbf{x}_i + b) \ge M\ \text{for all}\ i = 1 \dotsc m
  \end{aligned}
\end{equation}

Assuming that a solution to this optimization problem exists, it still has the issue that although the solution hyperplane is unique, there is an infinity of parameter vectors $w$ resulting in the solution hyperplane, only differing in their length. Uniqueness of the result might seem threatened, but as the $w$ only expresses the direction of the hyperplane and its margin, uniqueness can be ensured by adding the following constraint to its length

\begin{equation*}
  M\|w\| = 1
\end{equation*}
  
We can thus reformulate the optimization problem into the following quadratic optimization problem which is known to have a numerical solution

\begin{equation}
  \begin{aligned}
    &\underset{w \in \mathbb{R}^n, b \in \mathbb{R}} {\text{minimize}}
    & &\|w\|\\
    &\text{subject to}
    & &y_i(w \cdot \mathbf{x}_i + b) \ge 1\ \text{for all}\ i = 1 \dotsc m
  \end{aligned}
\end{equation}

\subsection {Support vectors}

Although the previous formulation of the optimization problem helped us to better understand the ideas of the linear SVMs, the resulting quadratic problem will become impractical when later introducing kernels methods. We will now reformulate an equvalent optimization problem (4) that will not only solve the computational issues, but will also give us more insights about the resulting solution.

In order to get to these benefits, we introduce the Lagrangian function $L$ together with Lagrange multipliers $\boldsymbol{\alpha}_i \ge 0$

\begin{equation}
  L(w, b, \boldsymbol{\alpha}) = \frac{1}{2}\|w\|^2 - \sum^m_{i=1} \alpha_i[y_i(w \cdot \mathbf{x}_i + b)) - 1]
\end{equation}

This is called the dual formulation of the original optimization problem, called the primal problem, in which $w$ and $b$ are called primal variables and $\boldsymbol{\alpha}$ the dual variable. The primal optimization problem is equivalent to finding a saddle point of $L$, minimizing with respect to $w$ and $b$, while maximizing with respect to $\boldsymbol{\alpha}$.

The solution to this problem is a saddle point, a different kind of extremum with respect to each of the variables. Because the solution is an extremum, we know that the partial derivative with respect to each of the variables will be zero. Thus the following equalities must hold

\begin{equation}
  \frac{\partial}{\partial b}L(w, b, \boldsymbol{\alpha}) = 0
  \ \text{and}\ 
  \frac{\partial}{\partial w}L(w, b, \boldsymbol{\alpha}) = 0
\end{equation}

Which simplifies to the following constraints 

\begin{equation}
  \sum^m_{i=1} \boldsymbol{\alpha}_iy_i = 0
  \ \text{and}\ 
  w = \sum^m_{i=1} \boldsymbol{\alpha}_iy_i\mathbf{x}_i
\end{equation}

By replacing $(6)$ and $(7)$ in the Lagrangian $(5)$, we obtain an optimization problem free of primal variables, and only optimize against the vector $\boldsymbol{\alpha}$

\begin{equation}
  \begin{aligned}
    &\underset{\boldsymbol{\alpha} \in \mathbb{R}^n} {\text{maximize}}
    & & W(\boldsymbol{\alpha}) = \sum_{i=1}^m\boldsymbol{\alpha}_i - \frac{1}{2}\sum_{i,j=1}^m\boldsymbol{\alpha}_i\boldsymbol{\alpha}_jy_iy_j\mathbf{x}_i \cdot \mathbf{x}_j\\
    &\text{subject to}
    & &\boldsymbol{\alpha} \ge 0\ \text{for all}\ i = 1 \dotsc m\\
    & & &\sum^m_{i=1} \boldsymbol{\alpha}_iy_i = 0
  \end{aligned}
\end{equation}

Finally, the Karush-Kuhn-Tucker condition \cite{kuhn1951}, from optimization theory, states that the solution $\boldsymbol{\alpha}$ satisfies the following equality for all $i = 1\dotsc m$

\begin{equation}
  \boldsymbol{\alpha}_i[y_i(w \cdot \mathbf{x}_i + b) - 1] = 0
\end{equation}

The importance of this equality is twofold. Firstly, it allows us from one observation $\mathbf{x}_j$ and its label $y_j$ for which the Lagrangian multiplier $\boldsymbol{\alpha}_j \neq 0$ to compute $b$ as following

\begin{equation}
  \begin{aligned}
    &\boldsymbol{\alpha}_j[y_j(w \cdot \mathbf{x}_j + b) - 1] = 0\\
    \Rightarrow\ &b = \frac{1}{y_j} - w \cdot \mathbf{x}_j
  \end{aligned}
\end{equation}

Secondly, this equality implies that every observation not lying on the margin must have a vanishing Lagrangian coefficient, meaning $\boldsymbol{\alpha}_i = 0$. These special observations are called \textit{support vectors}. As we have shown in (7) and (10), the parameters $w$ and $b$ that uniquely define the separating hyperplane can both be expressed only in terms of the support vectors. This means that support vectors carry all the information that was to be extracted from the training set in order to create the decision boundary. The decision function can be rewritten as following

\begin{equation}
  f(\mathbf{x}) = sgn\left(\sum_{\mathbf{x}_i \text{ is SV}}\boldsymbol{\alpha}_iy_i\mathbf{x} \cdot \mathbf{x}_i + b\right)
\end{equation}

Furthermore, it can be shown \cite{Vapnik:1995:NSL:211359} that the number of support vectors give us the following upper bound on the classifier's out of sample error. This means that a low number of support vectors can is an indicator of the capacity of a classifier to generalize to unseen samples. 

\begin{equation}
  \mathbb{E}\left[\text{Pr}\left(\text{error}\right)\right] \leq \frac{\mathbb{E}\left[\text{number of support vectors}\right]}{\text{number of training observations}}
\end{equation}

This inequality allows us to quickly derive a simple estimation of the quality of the learned classifier by just looking at the number of support vector.

\subsection{Towards SVMs}

In this work, we have chosen to present a simplified version of the linear SVM called the hard margin classifier. This was done in order to focus on building an intuition of the way linear SVMs work. In practice, several considerations have to be made in order to make this classifier usable.

The most important addition to the margin maximization algorithm that we have omitted so far is the ability to train on a data set where the constraint in our optimizing problem cannot be fulfilled. Indeed, real world datasets are often generated from measuring physical phenomena, this kind of dataset is subject to noise that can cause an overlapping of the two classes around the decision boundary, making it impossible to find a boundary properly separating the two classes of observations. Thankfully, these issues can be mitigated by introducing slack variables that will compensate the offset of an observation in the direction of the other class. A thorough investigation of this method can be found in \cite{Schoelkopf95extractingsupport}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_with_kernels"
%%% End:

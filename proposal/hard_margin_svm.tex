\section {Margin maximization classifier}

The hard margin classifier learns the parameters $w \in \mathbb{R}^n$ and $b \in \mathbb{R}$ of a hyperplane $\mathscr{H} := \left\{ \mathbf{x} \in \mathbb{R}^n\ |\ w \cdot \mathbf{x} + b = 0\right\}$, separating two classes of observations $\mathbf{x_i} \in \mathbb{R}^n$. To train this classifier, the algorithm takes a set of $m$ observations $\mathbf{x_i}$ together with a vector of labels $y_i = \pm 1$ for each of the observations indicating to which class belongs each observation 

\begin{equation*}
(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dotsc, (\mathbf{x}_m, y_m)
\end{equation*}

The learned parameters are then used to define a decision function $f$, assigning to points of $\mathbb{R}^n$ a label $\pm 1$ corresponding to the side of the hyperplane on which the observation stands

\begin{equation}
  f(\mathbf{x}) = sgn(w \cdot \mathbf{x} + b)
\end{equation}

\begin{figure*}
  \begin{minipage}{.5\textwidth}
    \centering
    \input{fig1}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \input{fig2}
  \end{minipage}

  \caption{
    Left side illustrates the idea behind margin maximization. Smaller points in the $\{ \mathbf{x} + M \}$ are generated by adding noise to an observation of the training set. The right figure shows a dataset separated by a hyperplane with parameters $w, b$. The norm of $w$ is determined by the distance from the plane to the support vectors.
  }
\end{figure*}

\subsection {Maximizing the margin}

One particularity about the SVM's learning algorithm is the objective function it tries to optimize. Other learning algorithms typically chose a loss function based on the empirical risk, defined as following for a classifier $f$ and a training set $(\mathbf{x}, \mathbf{y})$ of size $m$

\begin{equation}
  R_{\text{emp}}[f] = \frac{1}{m}\sum^m_{i=1}\frac{1}{2}|f(\mathbf{x}_i) - y_i|
\end{equation}

The margin maximization algorithm instead, searches within the space of hyperplanes properly classifying the training set for the hyperplane with the largest margin, distance from the training points to the hyperplane.

The choice of maximizing the margin has good theoretical roots. Indeed, it is possible to derive an upper bound on the expected misclassification rate of a linear classifier based on the margin of the underlying hyperplane to the training set. It is beyond the scope of this work to go into further details but we will still attempt to provide an intuition on why it might be interesting to optimize it, and for more information the reader can refer to \textcolor[rgb]{1,0,0}{ref?}.

The intuition comes from the fact that the observations from the test set, or encountered while using the classifier, have been generated by a similar process as the observations present in the training set. Thus, one can assume that observations are very similar to each other up to some noise. Maximizing the hyperplane margin
$M$ is thus the same as increasing the tolerance to noise of the classifier. As shown in figure 1, given an observation $\mathbf{x}_i$ from the training set, all new observations lying in the hypersphere of radius of the margin while be classified similarly to $\mathbf{x}_i$.

This idea translates well into the following optimization problem, trying to maximize the margin while constraining the search space to the set of classifiers property classifying the training set

\begin{equation}
  \begin{aligned}
    &\underset{w \in \mathbb{R}^n, b \in \mathbb{R}} {\text{maximize}}
    & & M := \frac{1}{\|w\|} \left[\underset{i} {\text{min}}\ 
    y_i(w \cdot \mathbf{x}_i + b)\right]\\
    &\text{subject to}
    & &y_i(w \cdot \mathbf{x}_i + b) \ge M\ \text{for all}\ i = 1 \dotsc m
  \end{aligned}
\end{equation}

Assuming that a solution to this optimization problem exists, it still has the issue that although the solution hyperplane is unique, there is an infinity of parameter vectors $w$ resulting in the solution hyperplane, only differing in their length. Uniqueness of the result might seem threatened, but as the $w$ is only relevant to expressing the direction of the hyperplane and its margin, uniqueness can be ensured by adding the following constraint to its length

\begin{equation*}
  M\|w\| = 1
\end{equation*}
  
We can thus reformulate the optimization problem into the following quadratic optimization problem which is known to have a numerical solution

\begin{equation}
  \begin{aligned}
    &\underset{w \in \mathbb{R}^n, b \in \mathbb{R}} {\text{minimize}}
    & &\|w\|\\
    &\text{subject to}
    & &y_i(w \cdot \mathbf{x}_i + b) \ge 1\ \text{for all}\ i = 1 \dotsc m
  \end{aligned}
\end{equation}

\subsection {Support vectors}

Although the previous formulation of the problem helped us to better understand the ideas of the linear SVMs, the resulting quadratic problem will become impractical when later introducing kernels methods. We will introduce in this section an equivalent formulation of the optimization problem (4) that will not only solve the computational issues, but will also give us more insights about the resulting solution.

In order to get to these benefits, we introduce the Lagrangian function $L$ together with Lagrange multipliers $\alpha_i \ge 0$

\begin{equation}
  L(w, b, \boldsymbol{\alpha}) = \frac{1}{2}\|w\|^2 - \sum^m_{i=1} \alpha_i[y_i(w \cdot \mathbf{x}_i + b)) - 1]
\end{equation}

This is called the dual formulation of the original optimization problem, called the primal problem, in which $w$ and $b$ are called primal variables and $\boldsymbol{\alpha}$ the dual variable. It can be shown \textcolor[rgb]{1,0,0}{ref?} that the primal optimization problem is equivalent to finding a saddle point of $L$, minimizing with respect to $w$ and $b$, while maximizing it with respect to $\boldsymbol{\alpha}$.

The solution to this problem will thus be a saddle point, a different kind of extremum with respect to each of the variables. Because the solution is an extremum, we know that the partial derivative with respect to each of the variables will be zero. Thus the following equations must hold

\begin{equation}
  \frac{\partial}{\partial b}L(w, b, \boldsymbol{\alpha}) = 0
  \ \text{and}\ 
  \frac{\partial}{\partial w}L(w, b, \boldsymbol{\alpha}) = 0
\end{equation}

Which simplifies to the following constraints 

\begin{equation}
  \sum^m_{i=1} \boldsymbol{\alpha}_iy_i = 0
  \ \text{and}\ 
  w = \sum^m_{i=1} \alpha_iy_i\mathbf{x}_i
\end{equation}

Further, by replacing $(6)$ and $(7)$ in the Lagrangian $(7)$, we obtain an optimization problem free of primal variables, and only optimize against the vector $\boldsymbol{\alpha}$

\begin{equation}
  \begin{aligned}
    &\underset{\boldsymbol{\alpha} \in \mathbb{R}^n} {\text{maximize}}
    & & W(\boldsymbol{\alpha}) = \sum_{i=1}^m\boldsymbol{\alpha}_i - \frac{1}{2}\sum_{i,j=1}^m\boldsymbol{\alpha}_i\boldsymbol{\alpha}_jy_iy_j\mathbf{x}_i \cdot \mathbf{x}_j\\
    &\text{subject to}
    & &\boldsymbol{\alpha} \ge 0\ \text{for all}\ i = 1 \dotsc m\\
    & & &\sum^m_{i=1} \boldsymbol{\alpha}_iy_i = 0
  \end{aligned}
\end{equation}

Finally, the Karush-Kuhn-Tucker theorem states that the solution $\boldsymbol{\alpha}$ satisfies the following equality for all $i = 1\dotsc m$
\textcolor[rgb]{1,0,0}{ref?}

\begin{equation}
  \boldsymbol{\alpha}_i[y_i(w \cdot \mathbf{x}_i + b) - 1] = 0
\end{equation}

The importance of this equality is twofold. First, it allows us to compute $b$ from one observation and its label $(\mathbf{x}_j, y_j)$ for which the Lagrangian multiplier $\boldsymbol{\alpha}_j \neq 0$ as following

\begin{equation}
  \begin{aligned}
    &\boldsymbol{\alpha}_j[y_j(w \cdot \mathbf{x}_j + b) - 1] = 0\\
    \Rightarrow\ &b = \frac{1}{y_j} - w \cdot \mathbf{x}_j
  \end{aligned}
\end{equation}

Second, this equality implies that every observations not lying on the margin must have a vanishing Lagrangian coefficient, meaning $\boldsymbol{\alpha}_i = 0$. These special observations are called \textit{support vectors}. As we have shown in (7) and (10), the parameters $b$ and $w$ that uniquely define the separating hyperplane can be expressed in terms of an expression only dependent on the support vectors. This means that all the information that was to be extracted from the training set in order to create the decision boundary was contained in the support vectors. Thus the new decision function can be rewritten as following

\begin{equation}
  f(\mathbf{x}) = sgn\left(\sum_{\mathbf{x}_i \text{ is SV}}\alpha_iy_i\mathbf{x} \cdot \mathbf{x}_i + b\right)
\end{equation}

Furthermore, it can be shown that the number of support vectors give us an upper bound on the out of sample error of the classifier. This means that a low number of support vectors can is an indicator of the capacity of a classifier to generalize to unseen samples. The upper bound is following \textcolor[rgb]{1,0,0}{ref?}

\begin{equation}
  E\left[Pr\left(\text{error}\right)\right] \leq \frac{E\left[\text{number of support vectors}\right]}{\text{number of training observations}}
\end{equation}

This inequality allows us to quickly derive a simple estimation of the quality of the learned classifier by just looking at the number of support vector.

\subsection{Towards SVMs}

We have chosen in this work to present a simplified version of the linear SVMs called the hard margin classifier. This was done in order to focus on building an intuition of the way linear SVMs work. In practice, several considerations have to be made in order to make this classifier usable.

The most important addition to the margin maximization algorithm that we have omitted so far is the ability to train on a data set where the constraint in our optimizing problem can't be fulfilled. Indeed, real world datasets are often generated from measuring physical phenomena, this kind of dataset is subject to noise that can cause an overlapping of the two classes around the decision boundary making it impossible to find a boundary properly separating the two classes of observations. Thankfully, these issues can be mitigated by introducing slack variables that will compensate the offset of an observation in the direction of the other class. A thorough investigation of this method can be found in \textcolor[rgb]{1,0,0}{ref?}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_with_kernels"
%%% End:
